{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "CYypzb0Z0IU-",
        "7EaT9wauzWBw"
      ],
      "authorship_tag": "ABX9TyOLh8M0KFVE8cXJ3+Ko/0hS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MiroxDot/Projects/blob/main/Diabetes_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Repository**"
      ],
      "metadata": {
        "id": "CYypzb0Z0IU-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kz7c4py0RtLi",
        "outputId": "7beab1ff-18d2-463f-be8b-7228b57e5478"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'hku_phys3151_2022' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/XiaoxueRan/hku_phys3151_2022"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from pandas import DataFrame\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import numpy as np\n",
        "from scipy.optimize import fmin_tnc\n",
        "\n",
        "class LogisticRegressionUsingGD:\n",
        "\n",
        "  # Static method to compute the sigmoid function, used to map predictions to probabilities\n",
        "  @staticmethod\n",
        "  def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "  # Static method for computing the net input, i.e., the dot product of input features and weights\n",
        "  @staticmethod\n",
        "  def net_input(theta, x):\n",
        "    return np.dot(x, theta)\n",
        "\n",
        "  # Calculates the probability of an input belonging to the positive class\n",
        "  def probability(self, theta, x):\n",
        "    return self.sigmoid(self.net_input(theta, x))\n",
        "\n",
        "  # Computes the logistic regression cost function\n",
        "  def cost_function(self, theta, x, y):\n",
        "    m = x.shape[0]  # number of samples\n",
        "    total_cost = -(1 / m) * np.sum(\n",
        "        y * np.log(self.probability(theta, x)) + (1 - y) * np.log(\n",
        "        1 - self.probability(theta, x)))\n",
        "    return total_cost\n",
        "\n",
        "  # Computes the gradient of the cost function to be used in the optimization algorithm\n",
        "  def gradient(self, theta, x, y):\n",
        "    m = x.shape[0]  # number of samples\n",
        "    return (1 / m) * np.dot(x.T, self.sigmoid(self.net_input(theta, x)) - y)\n",
        "\n",
        "  # Fits the logistic regression model to the data using gradient descent\n",
        "  def fit(self, x, y, theta):\n",
        "    opt_weights = fmin_tnc(func=self.cost_function, x0=theta, fprime=self.gradient, args=(x, y.flatten()))\n",
        "    self.w_ = opt_weights[0]\n",
        "    return self\n",
        "\n",
        "  # Predicts the class labels for samples in x\n",
        "  def predict(self, x):\n",
        "    theta = self.w_[:, np.newaxis]\n",
        "    return self.probability(theta, x)\n",
        "\n",
        "  # Calculates the model accuracy based on a probability threshold, usually 0.5 for binary classification\n",
        "  def accuracy(self, x, actual_classes, probab_threshold=0.5):\n",
        "    predicted_classes = (self.predict(x) >= probab_threshold).astype(int)\n",
        "    predicted_classes = predicted_classes.flatten()\n",
        "    accuracy = np.mean(predicted_classes == actual_classes)\n",
        "    return accuracy * 100\n",
        "\n"
      ],
      "metadata": {
        "id": "dw4_SIkBxwua"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Read Data**"
      ],
      "metadata": {
        "id": "-i4yY7eX1YeZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data from a CSV file located at the specified path using pandas' read_csv function\n",
        "df = pd.read_csv(\"/content/hku_phys3151_2022/logistic-regression/logistic-regression-example-1.csv\", sep=\",\")\n",
        "\n",
        "# Print the first 10 rows and the last 10 rows of the DataFrame to the console using concat instead of append\n",
        "# pd.concat is recommended as append is deprecated in newer versions of pandas\n",
        "print(pd.concat([df.head(10), df.tail(10)]))\n",
        "\n",
        "# Assign the DataFrame to a new variable 'data' for further manipulation or analysis\n",
        "data = df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kExn-_yzIt6",
        "outputId": "b4e157a1-7a9f-4c7d-f37a-1aa013055b2d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
            "0              6      148             72             35        0  33.6   \n",
            "1              1       85             66             29        0  26.6   \n",
            "2              8      183             64              0        0  23.3   \n",
            "3              1       89             66             23       94  28.1   \n",
            "4              0      137             40             35      168  43.1   \n",
            "5              5      116             74              0        0  25.6   \n",
            "6              3       78             50             32       88  31.0   \n",
            "7             10      115              0              0        0  35.3   \n",
            "8              2      197             70             45      543  30.5   \n",
            "9              8      125             96              0        0   0.0   \n",
            "758            1      106             76              0        0  37.5   \n",
            "759            6      190             92              0        0  35.5   \n",
            "760            2       88             58             26       16  28.4   \n",
            "761            9      170             74             31        0  44.0   \n",
            "762            9       89             62              0        0  22.5   \n",
            "763           10      101             76             48      180  32.9   \n",
            "764            2      122             70             27        0  36.8   \n",
            "765            5      121             72             23      112  26.2   \n",
            "766            1      126             60              0        0  30.1   \n",
            "767            1       93             70             31        0  30.4   \n",
            "\n",
            "     DiabetesPedigreeFunction  Age  Outcome  \n",
            "0                       0.627   50        1  \n",
            "1                       0.351   31        0  \n",
            "2                       0.672   32        1  \n",
            "3                       0.167   21        0  \n",
            "4                       2.288   33        1  \n",
            "5                       0.201   30        0  \n",
            "6                       0.248   26        1  \n",
            "7                       0.134   29        0  \n",
            "8                       0.158   53        1  \n",
            "9                       0.232   54        1  \n",
            "758                     0.197   26        0  \n",
            "759                     0.278   66        1  \n",
            "760                     0.766   22        0  \n",
            "761                     0.403   43        1  \n",
            "762                     0.142   33        0  \n",
            "763                     0.171   63        0  \n",
            "764                     0.340   27        0  \n",
            "765                     0.245   30        0  \n",
            "766                     0.349   47        1  \n",
            "767                     0.315   23        0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = data.iloc[:, :-1] # Features\n",
        "y = data.iloc[:, -1] # Outcome"
      ],
      "metadata": {
        "id": "ztjM_cZA8HY4"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add an intercept term (column of ones) to the feature matrix 'X'\n",
        "# This is essential for the logistic regression model to include a bias term (intercept)\n",
        "X = np.c_[np.ones((X.shape[0], 1)), X] # np.c_ concatenates along the second axis (columns)\n",
        "\n",
        "# Convert the target variable 'y' from a pandas Series to a numpy array and reshape it into a column vector\n",
        "# This ensures 'y' is in the correct shape for matrix operations that follow\n",
        "y = y.values.reshape((len(y), 1))\n",
        "\n",
        "# Initialize the parameter vector 'theta' with zeros for each feature (including the intercept)\n",
        "# The shape of 'theta' is determined by the number of features in 'X', ensuring compatibility for dot products\n",
        "theta = np.zeros((X.shape[1], 1))"
      ],
      "metadata": {
        "id": "jpfKKGba8mgP"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate an object of the LogisticRegressionUsingGD class\n",
        "model1 = LogisticRegressionUsingGD()\n",
        "\n",
        "# Fit the logistic regression model using the feature matrix X, target vector y, and initial parameter vector theta\n",
        "model1.fit(X, y, theta)\n",
        "\n",
        "# Calculate the accuracy of the model on the dataset X with targets y, after flattening y for compatibility\n",
        "accuracy = model1.accuracy(X, y.flatten())\n",
        "\n",
        "# Retrieve the parameter weights learned by the model during fitting\n",
        "parameters = model1.w_\n",
        "\n",
        "# Print the accuracy of the model using string formatting\n",
        "print(\"The accuracy of the model is {}\".format(accuracy))\n",
        "\n",
        "# Print the learned model parameters\n",
        "print(\"The model parameters got by Gradient descent:\")\n",
        "print(parameters)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zm_tV9b6_G-m",
        "outputId": "1684ad10-fa0b-48b4-abe0-4bae7a45a8cd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The accuracy of the model is 78.25520833333334\n",
            "The model parameters got by Gradient descent:\n",
            "[-8.33950666e+00  1.22992795e-01  3.49871049e-02 -1.33847683e-02\n",
            "  6.20495143e-04 -1.17722550e-03  8.89250124e-02  9.37136526e-01\n",
            "  1.46876967e-02]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a new input matrix 'X1' with two example feature sets\n",
        "# Each example set includes various feature values and a predefined intercept (the first element set to 1)\n",
        "# Features may represent different characteristics such as age, weight, blood pressure, etc.\n",
        "X1 = [[1, 6, 150, 72, 36, 170, 42, 1, 51], [1, 0, 85, 70, 24, 200, 25, 0.2, 25]]\n",
        "\n",
        "# Use the 'predict' method from the 'model1' instance (an instance of LogisticRegressionUsingGD)\n",
        "# to calculate the probability that each instance in 'X1' belongs to the positive class\n",
        "# The model outputs probabilities based on the logistic regression formula using the learned parameters\n",
        "print('The prediction of the first method:', model1.predict(X1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fv9lyNg6B1vo",
        "outputId": "ac18ee44-bb97-4d41-c5a0-602040565e73"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The prediction of the first method: [[0.87281508]\n",
            " [0.02308211]]\n"
          ]
        }
      ]
    }
  ]
}